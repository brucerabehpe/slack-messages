{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "falling-sunrise",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/lilmac/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'SnowBallStemmer' from 'nltk.stem' (/opt/anaconda3/lib/python3.8/site-packages/nltk/stem/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-128-ab3e50d239e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLancasterStemmer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSnowBallStemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSTOPWORDS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtextblob\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'SnowBallStemmer' from 'nltk.stem' (/opt/anaconda3/lib/python3.8/site-packages/nltk/stem/__init__.py)"
     ]
    }
   ],
   "source": [
    "# import basic libraries \n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk import download\n",
    "import itertools\n",
    "import collections\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer, PorterStemmer\n",
    "from nltk.stem import SnowBallStemmer\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "\n",
    "# read csv\n",
    "df = pd.read_csv('../data/data_clean.csv')\n",
    "df_copy = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expected-kingdom",
   "metadata": {},
   "source": [
    "### Sentiment scores in new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stunning-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sent = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "packed-devon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_links(df):\n",
    "#replace URL of a text\n",
    "    df_sent['text'] = df_sent['text'].str.replace('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ')\n",
    "\n",
    "clean_links(df_sent)\n",
    "df_sent['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "universal-reputation",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sent['text'] = df_sent['text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlled-desert",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = df_sent['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "together-afghanistan",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load VADER\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "# add VADER metrics to dataframe\n",
    "df_sent['compound'] = [analyzer.polarity_scores(v)['compound'] for v in text_list]\n",
    "df_sent['negative'] = [analyzer.polarity_scores(v)['neg'] for v in text_list]\n",
    "df_sent['neutral'] = [analyzer.polarity_scores(v)['neu'] for v in text_list]\n",
    "df_sent['positive'] = [analyzer.polarity_scores(v)['pos'] for v in text_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustainable-suggestion",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_sent.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statewide-throat",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to excel\n",
    "df_sent.to_excel(r'../data/data_clean_nlp.xlsx', index = False)\n",
    "# save to csv\n",
    "df_sent.to_csv(r'../data/data_clean_nlp.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vulnerable-illustration",
   "metadata": {},
   "source": [
    "### Something that works?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "centered-arctic",
   "metadata": {},
   "outputs": [],
   "source": [
    "Df_pd = pd.read_csv('../data/data_clean_nlp.csv',encoding = 'utf-8', header = None)\n",
    "Df_np = np.asarray(pd.read_csv('../data/data_clean_nlp.csv', encoding ='utf-8', header = None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advisory-history",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \" Hello world! I am going for coffee before work! \"\n",
    "sentences = sent_tokenize(paragraph)\n",
    "print(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "great-ecuador",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization_s(sentences): # same can be achieved for words tokens\n",
    "    s_new = []\n",
    "    for sent in (sentences[:][0]): #For NumpY = sentences[:]\n",
    "        s_token = sent_tokenize(sent)\n",
    "        if s_token != '':\n",
    "            s_new.append(s_token)\n",
    "    return s_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partial-investing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    clean_data = []\n",
    "    for x in (text[:][0]): #this is Df_pd for Df_np (text[:])\n",
    "        new_text = re.sub('<.*?>', '', x)   # remove HTML tags\n",
    "        new_text = re.sub(r'[^\\w\\s]', '', new_text) # remove punc.\n",
    "        new_text = re.sub(r'\\d+','',new_text)# remove numbers\n",
    "        new_text = new_text.lower() # lower case, .upper() for upper          \n",
    "        if new_text != '':\n",
    "            clean_data.append(new_text)\n",
    "    return clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "universal-quarter",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = \" Hello world! I am going for coffee before work! \"\n",
    "words = word_tokenize(sentences)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extreme-victim",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization_w(words):\n",
    "    w_new = []\n",
    "    for w in (words[:][0]):  # for NumPy = words[:]\n",
    "        w_token = word_tokenize(w)\n",
    "        if w_token != '':\n",
    "            w_new.append(w_token)\n",
    "    return w_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspected-voice",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowball = SnowballStemmer(language = 'english')\n",
    "def stemming(words):\n",
    "    new = []\n",
    "    stem_words = [snowball.stem(x) for x in (words[:][0])]\n",
    "    new.append(stem_words)\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vital-peoples",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supported-consortium",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "positive-closure",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deluxe-dress",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faced-sucking",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close-stable",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incident-coaching",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weekly-stewart",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "equal-brave",
   "metadata": {},
   "source": [
    "### Common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sensitive-lodge",
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "associate-lancaster",
   "metadata": {},
   "outputs": [],
   "source": [
    "download('stopwords')\n",
    "download('names')\n",
    "\n",
    "from nltk.corpus import stopwords, names\n",
    "from string import punctuation\n",
    "\n",
    "name_words = set([n.lower() for n in names.words()])\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def clean_words(words):\n",
    "    return [w for w in [w.lower() for w in words if w.isalpha()] if w not in stop_words and w not in name_words and w not in punctuation]\n",
    "\n",
    "def word_counts(words):\n",
    "    counts = {}\n",
    "    for word in words:\n",
    "        counts[word] = counts.get(word, 0) + 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proprietary-optimum",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data():\n",
    "    tokenized_text = [sent_tokenize(text) for text in a]\n",
    "    sentences = list(itertools.chain(*tokenized_text))\n",
    "    tokenized_words = [word_tokenize(sentence) for sentence in sentences]\n",
    "    words = list(itertools.chain(*tokenized_words))\n",
    "    return (sentences, words)\n",
    "\n",
    "main_sentences, main_words = get_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "macro-beauty",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designed-andrew",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import download\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# The SentimentIntensityAnalyzer model needs us to pull down the vader_lexicon\n",
    "download('vader_lexicon')\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "num_pos = 0\n",
    "num_neg = 0\n",
    "\n",
    "def is_positive(sentence: str) -> bool:\n",
    "    return sia.polarity_scores(sentence)[\"compound\"] > 0\n",
    "\n",
    "for s in main_sentences:\n",
    "    if is_positive(s):\n",
    "        num_pos +=1\n",
    "    else:\n",
    "        num_neg += 1\n",
    "        \n",
    "labels = 'Positive', 'Negative'\n",
    "sizes = [num_pos, num_neg]\n",
    "\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(sizes, labels=labels, autopct='%1.1f%%',\n",
    "        shadow=True, startangle=90)\n",
    "ax1.axis('equal')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "different-tumor",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = df['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excellent-fruit",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "important-hands",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bigger-sleep",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decimal-transcript",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "union-reggae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advisory-happiness",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "coral-hacker",
   "metadata": {},
   "source": [
    "### Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "registered-gross",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the text type to string\n",
    "df['text'] = df['text'].astype(str)\n",
    "\n",
    "# before lowercasing \n",
    "df['text'][2]\n",
    "\n",
    "# lowercase all reviews\n",
    "df['text'] = df['text'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "df['text'][2] ## to see the difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classified-small",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation\n",
    "def remove_punctuations(text):\n",
    "    for punctuation in string.punctuation:\n",
    "        text = text.replace(punctuation, '')\n",
    "    return text\n",
    "df['text'] = df['text'].apply(remove_punctuations)\n",
    "df['text'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "printable-suicide",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2894\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2895\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'text'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-125-d46fa54c6465>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# remove stopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mstop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2900\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2901\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2902\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2903\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2904\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2895\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'text'"
     ]
    }
   ],
   "source": [
    "# remove stopwords\n",
    "stop = stopwords.words('english')\n",
    "df['text'] = df['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "df['text'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liberal-attitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming\n",
    "#stemmer = PorterStemmer()\n",
    "#df['text'] = df['text'].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\n",
    "#df['text'][2]\n",
    "\n",
    "#df['text']=df['text'].apply(lambda x: \" \".join((stemmer.stem(i) for i in x.split())))\n",
    "#df['text'][2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "novel-daniel",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "stopwords = set(STOPWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earned-inflation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_wordcloud(col, title = None):\n",
    "    wordcloud = WordCloud(\n",
    "        background_color='white',\n",
    "        stopwords=stopwords,\n",
    "        max_words=500,\n",
    "        max_font_size=40, \n",
    "        scale=3,\n",
    "        random_state=1\n",
    "    ).generate(str(col))\n",
    "\n",
    "    fig = plt.figure(1, figsize=(14, 14))\n",
    "    plt.axis('off')\n",
    "    if title: \n",
    "        fig.suptitle(title, fontsize=20)\n",
    "        fig.subplots_adjust(top=2.3)\n",
    "\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    show_wordcloud(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suited-twist",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thrown-attribute",
   "metadata": {},
   "outputs": [],
   "source": [
    "# most common words\n",
    "import collections\n",
    "from collections import Counter\n",
    "import itertools\n",
    "\n",
    "\n",
    "# List of all words across tweets\n",
    "all_words = list(itertools.chain(*text_list))\n",
    "\n",
    "# Create counter\n",
    "counts = collections.Counter(all_words)\n",
    "\n",
    "counts.most_common(15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
